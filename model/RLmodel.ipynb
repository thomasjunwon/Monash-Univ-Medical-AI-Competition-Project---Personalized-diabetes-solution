{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "b234df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_pickle(f\"../input.pkl\")\n",
    "y = pd.read_pickle(f\"../output.pkl\")\n",
    "newdf=pd.concat([X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "4b5ef8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>educ</th>\n",
       "      <th>marry</th>\n",
       "      <th>house</th>\n",
       "      <th>pov</th>\n",
       "      <th>wt</th>\n",
       "      <th>ht</th>\n",
       "      <th>bmi</th>\n",
       "      <th>...</th>\n",
       "      <th>ldl</th>\n",
       "      <th>hdl</th>\n",
       "      <th>acratio</th>\n",
       "      <th>glu</th>\n",
       "      <th>insulin</th>\n",
       "      <th>crp</th>\n",
       "      <th>hb1ac</th>\n",
       "      <th>mvpa</th>\n",
       "      <th>ac_week</th>\n",
       "      <th>dbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>101.8</td>\n",
       "      <td>174.2</td>\n",
       "      <td>33.5</td>\n",
       "      <td>...</td>\n",
       "      <td>135.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.64</td>\n",
       "      <td>99.0</td>\n",
       "      <td>19.91</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5.6</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>17.307692</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>90.6</td>\n",
       "      <td>173.3</td>\n",
       "      <td>30.2</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.07</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>5.1</td>\n",
       "      <td>43.808976</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>76.7</td>\n",
       "      <td>177.3</td>\n",
       "      <td>24.4</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6.29</td>\n",
       "      <td>88.0</td>\n",
       "      <td>7.20</td>\n",
       "      <td>0.92</td>\n",
       "      <td>4.8</td>\n",
       "      <td>555.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.67</td>\n",
       "      <td>73.1</td>\n",
       "      <td>159.5</td>\n",
       "      <td>28.7</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12.41</td>\n",
       "      <td>107.0</td>\n",
       "      <td>9.81</td>\n",
       "      <td>5.22</td>\n",
       "      <td>5.8</td>\n",
       "      <td>844.615385</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>94.0</td>\n",
       "      <td>171.7</td>\n",
       "      <td>31.9</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.94</td>\n",
       "      <td>104.0</td>\n",
       "      <td>7.26</td>\n",
       "      <td>0.71</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1260.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>94.8</td>\n",
       "      <td>178.6</td>\n",
       "      <td>29.7</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>46.56</td>\n",
       "      <td>99.0</td>\n",
       "      <td>16.61</td>\n",
       "      <td>3.69</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1540.000000</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>42.8</td>\n",
       "      <td>155.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>...</td>\n",
       "      <td>133.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.4</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.74</td>\n",
       "      <td>68.5</td>\n",
       "      <td>158.5</td>\n",
       "      <td>27.3</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.29</td>\n",
       "      <td>97.0</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.67</td>\n",
       "      <td>5.4</td>\n",
       "      <td>447.692308</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.12</td>\n",
       "      <td>74.4</td>\n",
       "      <td>176.9</td>\n",
       "      <td>23.8</td>\n",
       "      <td>...</td>\n",
       "      <td>171.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.55</td>\n",
       "      <td>105.0</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.42</td>\n",
       "      <td>5.2</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>113.8</td>\n",
       "      <td>178.5</td>\n",
       "      <td>35.7</td>\n",
       "      <td>...</td>\n",
       "      <td>106.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.24</td>\n",
       "      <td>204.0</td>\n",
       "      <td>22.77</td>\n",
       "      <td>8.81</td>\n",
       "      <td>9.5</td>\n",
       "      <td>840.000000</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender   age  race  educ  marry  house   pov     wt     ht   bmi  ...  \\\n",
       "0       1.0  66.0   3.0   5.0    1.0    2.0  5.00  101.8  174.2  33.5  ...   \n",
       "1       1.0  34.0   1.0   4.0    1.0    3.0  1.33   90.6  173.3  30.2  ...   \n",
       "2       1.0  51.0   3.0   5.0    1.0    4.0  5.00   76.7  177.3  24.4  ...   \n",
       "3       1.0  47.0   1.0   3.0    1.0    2.0  1.67   73.1  159.5  28.7  ...   \n",
       "4       1.0  50.0   3.0   5.0    1.0    2.0  5.00   94.0  171.7  31.9  ...   \n",
       "..      ...   ...   ...   ...    ...    ...   ...    ...    ...   ...  ...   \n",
       "758     1.0  68.0   3.0   4.0    1.0    4.0  1.30   94.8  178.6  29.7  ...   \n",
       "759     2.0  70.0   3.0   5.0    2.0    1.0  5.00   42.8  155.4  17.7  ...   \n",
       "760     2.0  74.0   3.0   5.0    2.0    1.0  2.74   68.5  158.5  27.3  ...   \n",
       "761     1.0  57.0   3.0   5.0    1.0    3.0  4.12   74.4  176.9  23.8  ...   \n",
       "762     1.0  33.0   1.0   4.0    1.0    2.0  3.28  113.8  178.5  35.7  ...   \n",
       "\n",
       "       ldl   hdl  acratio    glu  insulin   crp  hb1ac         mvpa  \\\n",
       "0    135.0  60.0     6.64   99.0    19.91  2.03    5.6   450.000000   \n",
       "1    111.0  46.0     4.07  100.0    11.38  1.05    5.1    43.808976   \n",
       "2    120.0  48.0     6.29   88.0     7.20  0.92    4.8   555.000000   \n",
       "3    122.0  41.0    12.41  107.0     9.81  5.22    5.8   844.615385   \n",
       "4     65.0  49.0     4.94  104.0     7.26  0.71    5.3  1260.000000   \n",
       "..     ...   ...      ...    ...      ...   ...    ...          ...   \n",
       "758  150.0  47.0    46.56   99.0    16.61  3.69    5.4  1540.000000   \n",
       "759  133.0  81.0     8.00   82.0     1.35  0.66    5.4   260.000000   \n",
       "760   83.0  74.0     7.29   97.0     9.31  0.67    5.4   447.692308   \n",
       "761  171.0  55.0     4.55  105.0     6.60  0.42    5.2   300.000000   \n",
       "762  106.0  33.0     5.24  204.0    22.77  8.81    9.5   840.000000   \n",
       "\n",
       "       ac_week  dbs  \n",
       "0    17.307692  0.0  \n",
       "1     4.000000  0.0  \n",
       "2     0.576923  0.0  \n",
       "3     3.500000  0.0  \n",
       "4     2.000000  0.0  \n",
       "..         ...  ...  \n",
       "758   0.173077  0.0  \n",
       "759   1.000000  0.0  \n",
       "760   4.000000  0.0  \n",
       "761   7.000000  0.0  \n",
       "762   0.173077  2.0  \n",
       "\n",
       "[763 rows x 35 columns]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "50596575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>educ</th>\n",
       "      <th>marry</th>\n",
       "      <th>house</th>\n",
       "      <th>pov</th>\n",
       "      <th>wt</th>\n",
       "      <th>ht</th>\n",
       "      <th>bmi</th>\n",
       "      <th>...</th>\n",
       "      <th>ldl</th>\n",
       "      <th>hdl</th>\n",
       "      <th>acratio</th>\n",
       "      <th>glu</th>\n",
       "      <th>insulin</th>\n",
       "      <th>crp</th>\n",
       "      <th>hb1ac</th>\n",
       "      <th>mvpa</th>\n",
       "      <th>ac_week</th>\n",
       "      <th>dbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>75.1</td>\n",
       "      <td>161.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.01</td>\n",
       "      <td>163.0</td>\n",
       "      <td>22.65</td>\n",
       "      <td>0.42</td>\n",
       "      <td>6.6</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>59.4</td>\n",
       "      <td>156.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.69</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.33</td>\n",
       "      <td>0.71</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1260.000000</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>83.1</td>\n",
       "      <td>176.8</td>\n",
       "      <td>26.6</td>\n",
       "      <td>...</td>\n",
       "      <td>185.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>9.60</td>\n",
       "      <td>131.0</td>\n",
       "      <td>11.50</td>\n",
       "      <td>0.93</td>\n",
       "      <td>6.8</td>\n",
       "      <td>33.808976</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.64</td>\n",
       "      <td>74.3</td>\n",
       "      <td>167.6</td>\n",
       "      <td>26.5</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>24.56</td>\n",
       "      <td>242.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.55</td>\n",
       "      <td>7.8</td>\n",
       "      <td>28.808976</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>93.6</td>\n",
       "      <td>183.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>17.12</td>\n",
       "      <td>186.0</td>\n",
       "      <td>7.01</td>\n",
       "      <td>5.71</td>\n",
       "      <td>8.0</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.06</td>\n",
       "      <td>70.4</td>\n",
       "      <td>174.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>270.32</td>\n",
       "      <td>125.0</td>\n",
       "      <td>5.99</td>\n",
       "      <td>2.58</td>\n",
       "      <td>6.4</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.259615</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>87.8</td>\n",
       "      <td>170.3</td>\n",
       "      <td>30.3</td>\n",
       "      <td>...</td>\n",
       "      <td>119.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>10.85</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14.55</td>\n",
       "      <td>1.90</td>\n",
       "      <td>5.4</td>\n",
       "      <td>117.617952</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>64.5</td>\n",
       "      <td>160.7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>9.97</td>\n",
       "      <td>171.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.7</td>\n",
       "      <td>57.543596</td>\n",
       "      <td>34.615385</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>63.6</td>\n",
       "      <td>171.8</td>\n",
       "      <td>21.5</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>147.0</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.19</td>\n",
       "      <td>7.2</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.92</td>\n",
       "      <td>97.6</td>\n",
       "      <td>179.5</td>\n",
       "      <td>30.3</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>24.37</td>\n",
       "      <td>182.0</td>\n",
       "      <td>35.49</td>\n",
       "      <td>2.61</td>\n",
       "      <td>5.6</td>\n",
       "      <td>390.471807</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.16</td>\n",
       "      <td>108.6</td>\n",
       "      <td>176.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>...</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>16.91</td>\n",
       "      <td>297.0</td>\n",
       "      <td>23.35</td>\n",
       "      <td>0.81</td>\n",
       "      <td>9.3</td>\n",
       "      <td>535.235903</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>78.9</td>\n",
       "      <td>151.9</td>\n",
       "      <td>34.2</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.46</td>\n",
       "      <td>187.0</td>\n",
       "      <td>45.17</td>\n",
       "      <td>2.67</td>\n",
       "      <td>8.9</td>\n",
       "      <td>43.808976</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.56</td>\n",
       "      <td>72.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>28.1</td>\n",
       "      <td>...</td>\n",
       "      <td>134.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.77</td>\n",
       "      <td>157.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>3.80</td>\n",
       "      <td>7.3</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.62</td>\n",
       "      <td>78.7</td>\n",
       "      <td>178.3</td>\n",
       "      <td>24.8</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>37.89</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8.55</td>\n",
       "      <td>5.94</td>\n",
       "      <td>6.2</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>102.1</td>\n",
       "      <td>183.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.95</td>\n",
       "      <td>110.0</td>\n",
       "      <td>15.15</td>\n",
       "      <td>1.14</td>\n",
       "      <td>5.7</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>69.8</td>\n",
       "      <td>177.2</td>\n",
       "      <td>22.2</td>\n",
       "      <td>...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>4.11</td>\n",
       "      <td>192.0</td>\n",
       "      <td>4.18</td>\n",
       "      <td>1.06</td>\n",
       "      <td>6.4</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.82</td>\n",
       "      <td>151.3</td>\n",
       "      <td>176.6</td>\n",
       "      <td>48.5</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>10.45</td>\n",
       "      <td>173.0</td>\n",
       "      <td>23.55</td>\n",
       "      <td>1.26</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.411968</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.91</td>\n",
       "      <td>85.4</td>\n",
       "      <td>175.1</td>\n",
       "      <td>27.9</td>\n",
       "      <td>...</td>\n",
       "      <td>179.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>16.14</td>\n",
       "      <td>117.0</td>\n",
       "      <td>11.40</td>\n",
       "      <td>2.03</td>\n",
       "      <td>11.0</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>71.6</td>\n",
       "      <td>167.7</td>\n",
       "      <td>25.5</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>105.65</td>\n",
       "      <td>121.0</td>\n",
       "      <td>14.11</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.2</td>\n",
       "      <td>335.713464</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>2.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>93.0</td>\n",
       "      <td>171.5</td>\n",
       "      <td>31.6</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.95</td>\n",
       "      <td>97.0</td>\n",
       "      <td>10.09</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1380.000000</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>116.6</td>\n",
       "      <td>150.2</td>\n",
       "      <td>51.7</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>139.0</td>\n",
       "      <td>28.01</td>\n",
       "      <td>3.02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>129.3</td>\n",
       "      <td>181.1</td>\n",
       "      <td>39.4</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6.47</td>\n",
       "      <td>110.0</td>\n",
       "      <td>9.34</td>\n",
       "      <td>7.57</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1590.000000</td>\n",
       "      <td>0.432692</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>2.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>79.9</td>\n",
       "      <td>151.4</td>\n",
       "      <td>34.9</td>\n",
       "      <td>...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.52</td>\n",
       "      <td>123.0</td>\n",
       "      <td>9.35</td>\n",
       "      <td>8.36</td>\n",
       "      <td>6.8</td>\n",
       "      <td>540.000000</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.27</td>\n",
       "      <td>97.2</td>\n",
       "      <td>181.3</td>\n",
       "      <td>29.6</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>153.0</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.97</td>\n",
       "      <td>6.2</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>78.1</td>\n",
       "      <td>168.3</td>\n",
       "      <td>27.6</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>14.63</td>\n",
       "      <td>210.0</td>\n",
       "      <td>7.97</td>\n",
       "      <td>3.20</td>\n",
       "      <td>9.8</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>106.2</td>\n",
       "      <td>183.7</td>\n",
       "      <td>31.5</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.99</td>\n",
       "      <td>188.0</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>92.9</td>\n",
       "      <td>181.9</td>\n",
       "      <td>28.1</td>\n",
       "      <td>...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>29.14</td>\n",
       "      <td>124.0</td>\n",
       "      <td>14.06</td>\n",
       "      <td>4.76</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>83.5</td>\n",
       "      <td>174.3</td>\n",
       "      <td>27.5</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>159.0</td>\n",
       "      <td>7.62</td>\n",
       "      <td>1.12</td>\n",
       "      <td>7.5</td>\n",
       "      <td>506.307692</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>82.8</td>\n",
       "      <td>170.9</td>\n",
       "      <td>28.3</td>\n",
       "      <td>...</td>\n",
       "      <td>145.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.40</td>\n",
       "      <td>182.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>10.72</td>\n",
       "      <td>7.9</td>\n",
       "      <td>207.617952</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>100.8</td>\n",
       "      <td>175.2</td>\n",
       "      <td>32.8</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.90</td>\n",
       "      <td>110.0</td>\n",
       "      <td>31.41</td>\n",
       "      <td>1.65</td>\n",
       "      <td>5.9</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>145.4</td>\n",
       "      <td>166.3</td>\n",
       "      <td>52.6</td>\n",
       "      <td>...</td>\n",
       "      <td>125.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>8.46</td>\n",
       "      <td>111.0</td>\n",
       "      <td>58.45</td>\n",
       "      <td>25.05</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>98.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>33.8</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>18.92</td>\n",
       "      <td>130.0</td>\n",
       "      <td>34.55</td>\n",
       "      <td>4.34</td>\n",
       "      <td>7.0</td>\n",
       "      <td>825.000000</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.16</td>\n",
       "      <td>124.7</td>\n",
       "      <td>171.2</td>\n",
       "      <td>42.5</td>\n",
       "      <td>...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>42.60</td>\n",
       "      <td>281.0</td>\n",
       "      <td>20.20</td>\n",
       "      <td>5.68</td>\n",
       "      <td>9.6</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.79</td>\n",
       "      <td>162.0</td>\n",
       "      <td>33.40</td>\n",
       "      <td>1.72</td>\n",
       "      <td>5.8</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.29</td>\n",
       "      <td>109.3</td>\n",
       "      <td>166.2</td>\n",
       "      <td>39.6</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>149.0</td>\n",
       "      <td>10.92</td>\n",
       "      <td>8.13</td>\n",
       "      <td>7.1</td>\n",
       "      <td>288.089758</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.42</td>\n",
       "      <td>94.0</td>\n",
       "      <td>158.6</td>\n",
       "      <td>37.4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>121.91</td>\n",
       "      <td>142.0</td>\n",
       "      <td>13.25</td>\n",
       "      <td>1.36</td>\n",
       "      <td>7.3</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.730769</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>2.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>89.3</td>\n",
       "      <td>162.4</td>\n",
       "      <td>33.9</td>\n",
       "      <td>...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.89</td>\n",
       "      <td>108.0</td>\n",
       "      <td>12.52</td>\n",
       "      <td>3.30</td>\n",
       "      <td>6.4</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.46</td>\n",
       "      <td>109.2</td>\n",
       "      <td>183.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>...</td>\n",
       "      <td>147.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.86</td>\n",
       "      <td>133.0</td>\n",
       "      <td>8.37</td>\n",
       "      <td>1.25</td>\n",
       "      <td>6.8</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.46</td>\n",
       "      <td>175.8</td>\n",
       "      <td>186.5</td>\n",
       "      <td>50.5</td>\n",
       "      <td>...</td>\n",
       "      <td>105.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>12.08</td>\n",
       "      <td>129.0</td>\n",
       "      <td>17.51</td>\n",
       "      <td>2.66</td>\n",
       "      <td>6.7</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>2.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.21</td>\n",
       "      <td>97.7</td>\n",
       "      <td>149.2</td>\n",
       "      <td>43.9</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>125.0</td>\n",
       "      <td>12.86</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.1</td>\n",
       "      <td>70.195627</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>118.5</td>\n",
       "      <td>184.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>154.0</td>\n",
       "      <td>11.56</td>\n",
       "      <td>0.80</td>\n",
       "      <td>6.7</td>\n",
       "      <td>317.307692</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>113.6</td>\n",
       "      <td>171.5</td>\n",
       "      <td>38.6</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.76</td>\n",
       "      <td>91.0</td>\n",
       "      <td>18.89</td>\n",
       "      <td>3.11</td>\n",
       "      <td>5.8</td>\n",
       "      <td>82.853855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>77.3</td>\n",
       "      <td>147.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>...</td>\n",
       "      <td>126.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>22.63</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.46</td>\n",
       "      <td>11.89</td>\n",
       "      <td>5.1</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>113.8</td>\n",
       "      <td>178.5</td>\n",
       "      <td>35.7</td>\n",
       "      <td>...</td>\n",
       "      <td>106.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.24</td>\n",
       "      <td>204.0</td>\n",
       "      <td>22.77</td>\n",
       "      <td>8.81</td>\n",
       "      <td>9.5</td>\n",
       "      <td>840.000000</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender   age  race  educ  marry  house   pov     wt     ht   bmi  ...  \\\n",
       "9       1.0  57.0   2.0   2.0    1.0    2.0  2.73   75.1  161.0  29.0  ...   \n",
       "43      2.0  67.0   2.0   3.0    2.0    1.0  4.05   59.4  156.2  24.3  ...   \n",
       "59      1.0  66.0   3.0   4.0    1.0    4.0  5.00   83.1  176.8  26.6  ...   \n",
       "97      1.0  76.0   3.0   3.0    1.0    2.0  1.64   74.3  167.6  26.5  ...   \n",
       "109     1.0  55.0   3.0   5.0    2.0    1.0  5.00   93.6  183.0  27.9  ...   \n",
       "116     1.0  78.0   4.0   5.0    1.0    2.0  2.06   70.4  174.9  23.0  ...   \n",
       "121     2.0  58.0   3.0   5.0    2.0    1.0  5.00   87.8  170.3  30.3  ...   \n",
       "122     2.0  40.0   3.0   4.0    2.0    2.0  3.60   64.5  160.7  25.0  ...   \n",
       "132     1.0  61.0   2.0   5.0    1.0    4.0  5.00   63.6  171.8  21.5  ...   \n",
       "159     1.0  79.0   3.0   5.0    1.0    2.0  4.92   97.6  179.5  30.3  ...   \n",
       "173     1.0  51.0   4.0   4.0    1.0    5.0  2.16  108.6  176.3  34.9  ...   \n",
       "176     2.0  56.0   6.0   4.0    3.0    1.0  5.00   78.9  151.9  34.2  ...   \n",
       "185     2.0  67.0   3.0   4.0    1.0    2.0  4.56   72.4  160.4  28.1  ...   \n",
       "188     1.0  65.0   3.0   4.0    1.0    2.0  2.62   78.7  178.3  24.8  ...   \n",
       "221     1.0  57.0   2.0   4.0    1.0    2.0  5.00  102.1  183.4  30.4  ...   \n",
       "242     1.0  48.0   3.0   5.0    1.0    2.0  5.00   69.8  177.2  22.2  ...   \n",
       "280     1.0  58.0   3.0   4.0    1.0    2.0  4.82  151.3  176.6  48.5  ...   \n",
       "288     1.0  23.0   2.0   2.0    3.0    2.0  1.91   85.4  175.1  27.9  ...   \n",
       "298     1.0  72.0   3.0   5.0    2.0    1.0  1.84   71.6  167.7  25.5  ...   \n",
       "329     2.0  63.0   3.0   5.0    3.0    2.0  3.90   93.0  171.5  31.6  ...   \n",
       "342     2.0  57.0   4.0   2.0    2.0    1.0  1.10  116.6  150.2  51.7  ...   \n",
       "345     1.0  31.0   3.0   4.0    3.0    1.0  3.75  129.3  181.1  39.4  ...   \n",
       "350     2.0  58.0   4.0   4.0    1.0    2.0  5.00   79.9  151.4  34.9  ...   \n",
       "356     1.0  65.0   3.0   4.0    1.0    2.0  1.27   97.2  181.3  29.6  ...   \n",
       "368     1.0  63.0   3.0   3.0    1.0    3.0  4.44   78.1  168.3  27.6  ...   \n",
       "383     1.0  60.0   1.0   3.0    1.0    3.0  5.00  106.2  183.7  31.5  ...   \n",
       "385     1.0  63.0   3.0   4.0    1.0    3.0  1.89   92.9  181.9  28.1  ...   \n",
       "406     1.0  62.0   7.0   5.0    2.0    1.0  5.00   83.5  174.3  27.5  ...   \n",
       "422     2.0  53.0   4.0   5.0    1.0    2.0  5.00   82.8  170.9  28.3  ...   \n",
       "428     1.0  55.0   2.0   3.0    1.0    7.0  1.55  100.8  175.2  32.8  ...   \n",
       "433     2.0  26.0   3.0   4.0    3.0    5.0  1.57  145.4  166.3  52.6  ...   \n",
       "470     2.0  32.0   1.0   3.0    1.0    6.0  0.69   98.0  170.2  33.8  ...   \n",
       "511     1.0  68.0   3.0   3.0    1.0    2.0  2.16  124.7  171.2  42.5  ...   \n",
       "551     1.0  52.0   3.0   4.0    3.0    1.0  5.00   95.0  177.0  30.3  ...   \n",
       "555     2.0  65.0   3.0   4.0    1.0    2.0  2.29  109.3  166.2  39.6  ...   \n",
       "570     2.0  55.0   2.0   5.0    3.0    1.0  4.42   94.0  158.6  37.4  ...   \n",
       "578     2.0  63.0   3.0   5.0    1.0    3.0  5.00   89.3  162.4  33.9  ...   \n",
       "637     1.0  64.0   4.0   3.0    1.0    6.0  2.46  109.2  183.0  32.6  ...   \n",
       "700     1.0  58.0   3.0   5.0    3.0    1.0  4.46  175.8  186.5  50.5  ...   \n",
       "717     2.0  73.0   7.0   5.0    2.0    1.0  2.21   97.7  149.2  43.9  ...   \n",
       "718     1.0  64.0   3.0   5.0    1.0    3.0  5.00  118.5  184.3  34.9  ...   \n",
       "732     2.0  67.0   4.0   2.0    2.0    1.0  1.51  113.6  171.5  38.6  ...   \n",
       "755     2.0  44.0   1.0   2.0    3.0    6.0  0.32   77.3  147.8  35.4  ...   \n",
       "762     1.0  33.0   1.0   4.0    1.0    2.0  3.28  113.8  178.5  35.7  ...   \n",
       "\n",
       "       ldl   hdl  acratio    glu  insulin    crp  hb1ac         mvpa  \\\n",
       "9    101.0  53.0     5.01  163.0    22.65   0.42    6.6   330.000000   \n",
       "43    86.0  58.0     8.69  100.0     7.33   0.71    6.4  1260.000000   \n",
       "59   185.0  41.0     9.60  131.0    11.50   0.93    6.8    33.808976   \n",
       "97    68.0  51.0    24.56  242.0     7.90   0.55    7.8    28.808976   \n",
       "109   99.0  57.0    17.12  186.0     7.01   5.71    8.0   315.000000   \n",
       "116   54.0  72.0   270.32  125.0     5.99   2.58    6.4    90.000000   \n",
       "121  119.0  66.0    10.85  100.0    14.55   1.90    5.4   117.617952   \n",
       "122   49.0  76.0     9.97  171.0    18.00   0.92    7.7    57.543596   \n",
       "132   76.0  36.0     7.25  147.0     4.54   0.19    7.2   600.000000   \n",
       "159   58.0  50.0    24.37  182.0    35.49   2.61    5.6   390.471807   \n",
       "173  179.0  51.0    16.91  297.0    23.35   0.81    9.3   535.235903   \n",
       "176   73.0  35.0     7.46  187.0    45.17   2.67    8.9    43.808976   \n",
       "185  134.0  62.0    17.77  157.0     8.50   3.80    7.3   380.000000   \n",
       "188   79.0  35.0    37.89  102.0     8.55   5.94    6.2   720.000000   \n",
       "221   65.0  39.0     5.95  110.0    15.15   1.14    5.7   380.000000   \n",
       "242   81.0  72.0     4.11  192.0     4.18   1.06    6.4   600.000000   \n",
       "280   39.0  41.0    10.45  173.0    23.55   1.26    8.0    18.411968   \n",
       "288  179.0  47.0    16.14  117.0    11.40   2.03   11.0   510.000000   \n",
       "298   79.0  48.0   105.65  121.0    14.11   0.49    6.2   335.713464   \n",
       "329   59.0  48.0     9.95   97.0    10.09   2.94    5.9  1380.000000   \n",
       "342   82.0  54.0     4.44  139.0    28.01   3.02    7.0   700.000000   \n",
       "345  120.0  38.0     6.47  110.0     9.34   7.57    7.6  1590.000000   \n",
       "350  123.0  52.0     4.52  123.0     9.35   8.36    6.8   540.000000   \n",
       "356  190.0  56.0     5.05  153.0     8.83   0.97    6.2   180.000000   \n",
       "368   98.0  41.0    14.63  210.0     7.97   3.20    9.8   480.000000   \n",
       "383   76.0  35.0     7.99  188.0    12.90   0.57    8.0  1800.000000   \n",
       "385  110.0  41.0    29.14  124.0    14.06   4.76    5.8  1920.000000   \n",
       "406   76.0  35.0     0.64  159.0     7.62   1.12    7.5   506.307692   \n",
       "422  145.0  51.0     6.40  182.0    19.00  10.72    7.9   207.617952   \n",
       "428   51.0  45.0     8.90  110.0    31.41   1.65    5.9   360.000000   \n",
       "433  125.0  41.0     8.46  111.0    58.45  25.05    7.0   300.000000   \n",
       "470   71.0  36.0    18.92  130.0    34.55   4.34    7.0   825.000000   \n",
       "511   88.0  44.0    42.60  281.0    20.20   5.68    9.6   180.000000   \n",
       "551   76.0  35.0     5.79  162.0    33.40   1.72    5.8   120.000000   \n",
       "555   80.0  66.0    13.00  149.0    10.92   8.13    7.1   288.089758   \n",
       "570   85.0  42.0   121.91  142.0    13.25   1.36    7.3   180.000000   \n",
       "578   93.0  37.0     5.89  108.0    12.52   3.30    6.4   360.000000   \n",
       "637  147.0  54.0     4.86  133.0     8.37   1.25    6.8   240.000000   \n",
       "700  105.0  36.0    12.08  129.0    17.51   2.66    6.7   240.000000   \n",
       "717   94.0  72.0     4.22  125.0    12.86   6.70    6.1    70.195627   \n",
       "718   83.0  60.0     6.30  154.0    11.56   0.80    6.7   317.307692   \n",
       "732   57.0  50.0     4.76   91.0    18.89   3.11    5.8    82.853855   \n",
       "755  126.0  52.0    22.63  100.0    37.46  11.89    5.1   900.000000   \n",
       "762  106.0  33.0     5.24  204.0    22.77   8.81    9.5   840.000000   \n",
       "\n",
       "       ac_week  dbs  \n",
       "9    21.000000  2.0  \n",
       "43    0.028846  2.0  \n",
       "59    4.000000  2.0  \n",
       "97    0.173077  2.0  \n",
       "109  10.500000  2.0  \n",
       "116   0.259615  2.0  \n",
       "121   6.000000  2.0  \n",
       "122  34.615385  2.0  \n",
       "132   0.576923  2.0  \n",
       "159   2.000000  2.0  \n",
       "173   2.000000  2.0  \n",
       "176   0.028846  2.0  \n",
       "185   2.000000  2.0  \n",
       "188   4.000000  2.0  \n",
       "221   0.576923  2.0  \n",
       "242   0.086538  2.0  \n",
       "280   0.692308  2.0  \n",
       "288   4.000000  2.0  \n",
       "298   2.000000  2.0  \n",
       "329   0.028846  2.0  \n",
       "342   0.230769  2.0  \n",
       "345   0.432692  2.0  \n",
       "350   0.086538  2.0  \n",
       "356   6.000000  2.0  \n",
       "368   0.028846  2.0  \n",
       "383   1.153846  2.0  \n",
       "385   0.519231  2.0  \n",
       "406   0.086538  2.0  \n",
       "422   0.230769  2.0  \n",
       "428   0.865385  2.0  \n",
       "433   0.173077  2.0  \n",
       "470   0.057692  2.0  \n",
       "511   0.057692  2.0  \n",
       "551   0.173077  2.0  \n",
       "555   1.153846  2.0  \n",
       "570   1.730769  2.0  \n",
       "578   4.000000  2.0  \n",
       "637   1.000000  2.0  \n",
       "700   7.000000  2.0  \n",
       "717   0.173077  2.0  \n",
       "718   7.000000  2.0  \n",
       "732   1.000000  2.0  \n",
       "755   6.000000  2.0  \n",
       "762   0.173077  2.0  \n",
       "\n",
       "[44 rows x 35 columns]"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf[newdf['dbs']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "7274816b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (1.0, 2.0),\n",
       " 1: (20.0, 80.0),\n",
       " 2: (1.0, 7.0),\n",
       " 3: (1.0, 5.0),\n",
       " 4: (1.0, 3.0),\n",
       " 5: (1.0, 7.0),\n",
       " 6: (5.397605346934028e-79, 5.0),\n",
       " 7: (42.8, 175.8),\n",
       " 8: (140.3, 196.6),\n",
       " 9: (17.5, 56.7),\n",
       " 10: (63.7, 157.4),\n",
       " 11: (77.8, 157.2),\n",
       " 12: (38.0, 134.0),\n",
       " 13: (36.0, 111.0),\n",
       " 14: (87.0, 198.0),\n",
       " 15: (5.0, 148.0),\n",
       " 16: (2.8, 5.3),\n",
       " 17: (9.0, 103.0),\n",
       " 18: (0.39, 1.71),\n",
       " 19: (96.0, 309.0),\n",
       " 20: (24.0, 379.0),\n",
       " 21: (5.0, 191.0),\n",
       " 22: (2.2, 13.5),\n",
       " 23: (9.8, 17.7),\n",
       " 24: (30.6, 51.6),\n",
       " 25: (30.0, 200.0),\n",
       " 26: (23.0, 112.0),\n",
       " 27: (0.22, 270.32),\n",
       " 28: (59.0, 325.0),\n",
       " 29: (0.35, 96.13),\n",
       " 30: (0.11, 46.18),\n",
       " 31: (3.8, 11.6),\n",
       " 32: (4.423076923076923, 1920.0),\n",
       " 33: (0.028846153846153848, 56.15384615384615),\n",
       " 34: (0.0, 2.0)}"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_minmax = {\n",
    "    idx: (newdf.iloc[:, idx].min(), newdf.iloc[:, idx].max())\n",
    "    for idx in range(newdf.shape[1])\n",
    "}\n",
    "col_minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "9d3ef06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "d9f5b3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender       2.000000\n",
       "age         68.000000\n",
       "race         4.000000\n",
       "educ         4.000000\n",
       "marry        2.000000\n",
       "house        1.000000\n",
       "pov          2.130000\n",
       "wt          77.400000\n",
       "ht         164.800000\n",
       "bmi         28.500000\n",
       "wst        101.300000\n",
       "hip        102.700000\n",
       "dia         71.000000\n",
       "pulse       90.000000\n",
       "sys        131.000000\n",
       "alt         26.000000\n",
       "albumin      3.900000\n",
       "ast         25.000000\n",
       "crea         0.700000\n",
       "chol       125.000000\n",
       "tyg        192.000000\n",
       "ggt         21.000000\n",
       "wbc          6.100000\n",
       "hb          13.300000\n",
       "hct         39.800000\n",
       "ldl         58.000000\n",
       "hdl         35.000000\n",
       "acratio      1.000000\n",
       "glu        116.000000\n",
       "insulin     25.110000\n",
       "crp         10.820000\n",
       "hb1ac        6.600000\n",
       "mvpa       660.000000\n",
       "ac_week      0.086538\n",
       "Name: 23, dtype: float64"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "8f8b0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alt    148.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['alt']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "720697ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "8ed3ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader,Subset\n",
    "from lightning.pytorch import LightningModule,Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# 머신러닝 관련 라이브러리\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#기억력 관련 변수 ->cerad 4,6,7,8(8,10,11,12)\n",
    "#사고력 관련 변수 ->cerad 1,2,3,5,9,10,11,12 (5,6,7,9,13,14,15,16)\n",
    "# 나머지 변수 -> age,gender, edu,s_adl,s_iadl  (0,1,2,3,4)\n",
    "# 범주형 파생변수 -> j1v, 2,5,6,9,10,11,adl,iadl\n",
    "\n",
    "\n",
    "def to_tensor(*dfs):\n",
    "    return [torch.tensor(df.values,dtype=torch.float32) for df in dfs]\n",
    "\n",
    "\n",
    "\n",
    "class MainModule(nn.Module):\n",
    "    def __init__(self,input_dim=6,output_dim=3,do1=0.5,activ=0):\n",
    "        super().__init__()\n",
    "        self.activ=nn.GELU() if activ==1 else nn.ReLU()\n",
    "        self.input_layer=nn.Sequential(\n",
    "            nn.Linear(input_dim,6),\n",
    "            self.activ,\n",
    "            nn.Dropout(do1),\n",
    "            \n",
    "            nn.Linear(6,4),\n",
    "            self.activ,\n",
    "            nn.Dropout(do1),\n",
    "\n",
    "            nn.Linear(4,3),\n",
    "            self.activ,\n",
    "            nn.Dropout(do1),\n",
    "        )\n",
    "        \n",
    "        self.output_layer=nn.Linear(3,output_dim)\n",
    "    def forward(self,x):\n",
    "        x=self.input_layer(x)\n",
    "        output=self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "class SubModule(nn.Module):\n",
    "    def __init__(self,input_dim=28,output_dim=7,do1=0.5,activ=0):\n",
    "        super().__init__()\n",
    "        self.activ=nn.GELU() if activ==1 else nn.ReLU()\n",
    "        self.input_layer=nn.Sequential(\n",
    "            nn.Linear(input_dim,28),\n",
    "            self.activ,\n",
    "            nn.Dropout(do1),\n",
    "            \n",
    "            nn.Linear(28,14),\n",
    "            self.activ,\n",
    "            nn.Dropout(do1),\n",
    "\n",
    "            nn.Linear(14,7),\n",
    "            self.activ,\n",
    "            nn.Dropout(do1),\n",
    "        )\n",
    "        \n",
    "        self.output_layer=nn.Linear(7,output_dim)\n",
    "    def forward(self,x):\n",
    "        x=self.input_layer(x)\n",
    "        output=self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "class TotalModel(nn.Module):    #TotalModel의 128 dim에서 skip connection 적용\n",
    "    def __init__(self,do1,do2,activ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activ=nn.GELU() if activ==1 else nn.ReLU()\n",
    "\n",
    "        self.module1=SubModule(6,3,do1,activ)\n",
    "        self.module2=SubModule(28,7,do1,activ)\n",
    "\n",
    "        \n",
    "        self.hidden_layers=nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(10,10),\n",
    "                self.activ,\n",
    "                nn.Dropout(do2),\n",
    "            )for _ in range(1)\n",
    "        ])\n",
    "        \n",
    "        self.output_layers=nn.Sequential(\n",
    "            nn.Linear(10,5),\n",
    "            self.activ,\n",
    "            nn.Dropout(do2),\n",
    "            \n",
    "            nn.Linear(5,5),\n",
    "            self.activ,\n",
    "            nn.Dropout(do2),            \n",
    "            \n",
    "            nn.Linear(5,3)         \n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x1=x[:,[0,1,2,4,28,31]]\n",
    "        x2=x[:,[3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,29,30,32,33]]\n",
    "\n",
    "        out1=self.module1(x1)        #특정 4개의 열 데이터는 module1에 통과 -> output1 나온다      -shape:[batch_size,16]\n",
    "        out2=self.module2(x2)        #나머지 13개의 열 데이터는 module2에 통과 -> output2 나온다   -shape:[batch_size,16]\n",
    "\n",
    "        out=torch.cat([out1,out2],dim=1)       #-shape:[batch_size,32]\n",
    "        k=out\n",
    "        residual=k          #초기 결과를 k로 잔차 저장\n",
    "        for layer in self.hidden_layers:\n",
    "            j=layer(k)\n",
    "            k=j+residual\n",
    "            \n",
    "        output=self.output_layers(k)\n",
    "        return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "cdc1c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.99999342074517"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(1)\n",
    "\n",
    "def load_best_model(model, device=device):\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "patient_dict = {'gender': 1.0, 'age': 33.0, 'race': 1.0, 'educ': 4.0, 'marry': 1.0, 'house': 2.0, 'pov': 3.28, 'wt': 113.8, \n",
    "                'ht': 178.5, 'bmi': 35.7, 'wst': 121.8, 'hip': 113.3, 'dia': 79.0, 'pulse': 73.0, 'sys': 114.0, 'alt': 62.0, 'albumin': 4.0, \n",
    "                'ast': 32.0, 'crea': 0.82, 'chol': 170.0, 'tyg': 168.0, 'ggt': 36.0, 'wbc': 6.7, 'hb': 15.5, 'hct': 46.3, 'ldl': 106.0, \n",
    "                'hdl': 33.0, 'acratio': 5.24, 'glu': 204.0, 'insulin': 22.77, 'crp': 8.81, 'hb1ac': 9.5, 'mvpa': 840.0, 'ac_week': 0.173077, \n",
    "                'dbs': 2.0}\n",
    "\n",
    "def scoring2(patient_dict):\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    import pandas as pd\n",
    "\n",
    "    columns = ['gender', 'age', 'race', 'educ', 'marry', 'house', 'pov', 'wt', 'ht',\n",
    "               'bmi', 'wst', 'hip', 'dia', 'pulse', 'sys', 'alt', 'albumin', 'ast',\n",
    "               'crea', 'chol', 'tyg', 'ggt', 'wbc', 'hb', 'hct', 'ldl', 'hdl',\n",
    "               'acratio', 'glu', 'insulin', 'crp', 'hb1ac', 'mvpa', 'ac_week']\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df_patient = pd.DataFrame([[patient_dict[col] for col in columns]], columns=columns)\n",
    "    \n",
    "    # Tensor로 변환, 항상 2D로 (batch, features)\n",
    "    X_patient_t = torch.tensor(df_patient.values, dtype=torch.float32)\n",
    "    if X_patient_t.ndim == 1:\n",
    "        X_patient_t = X_patient_t.unsqueeze(0)\n",
    "\n",
    "    # 모델 불러오기\n",
    "    model = load_best_model(TotalModel(0.35, 0.35, 1))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(X_patient_t.to(device))   # [1, 3]\n",
    "        prob = torch.softmax(output, dim=1)      # [1,3]\n",
    "        prob = prob.squeeze(0)                    # [3]\n",
    "\n",
    "    # scoring 계산\n",
    "    score = (prob[1].item() + prob[2].item() * 2) * 50\n",
    "    return score\n",
    "\n",
    "\n",
    "scoring2(patient_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "ec1d4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(state_input):\n",
    "    \n",
    "    X_patient_t = state_input.float()\n",
    "    if X_patient_t.ndim == 1:\n",
    "        X_patient_t = X_patient_t.unsqueeze(0) \n",
    "\n",
    "    # 모델 불러오기\n",
    "    model = load_best_model(TotalModel(0.35, 0.35, 1))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(X_patient_t.to(device))   # [1, 3]\n",
    "        prob = torch.softmax(output, dim=1)      # [1,3]\n",
    "        prob = prob.squeeze(0)                    # [3]\n",
    "\n",
    "    # scoring 계산\n",
    "    score = (prob[1].item() + prob[2].item() * 2) * 50\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33208ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "da0eb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ======= 1. 환경 정의 =======\n",
    "class PatientEnv:\n",
    "    def __init__(self, patient_data, score_model, max_steps=5):\n",
    "        \"\"\"\n",
    "        patient_data: dict {patient_id: {\"features\": np.array, \"label\": int}}\n",
    "        score_model: scoring DNN 모델 (reward용)\n",
    "        \"\"\"\n",
    "        self.patient_data = patient_data\n",
    "        self.score_model = score_model\n",
    "        self.max_steps = max_steps\n",
    "        self.num_features = list(patient_data.values())[0]['features'].shape[0]\n",
    "\n",
    "    def reset(self, patient_id):  #pid를 받고 해당 환자의 변수정보를 state에 저장\n",
    "        self.patient = self.patient_data[patient_id]\n",
    "        self.state = self.patient['features'].copy()\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        return self.state.copy()\n",
    "\n",
    "    def step(self, action_idx, delta, alpha=100):\n",
    "        \"\"\"\n",
    "        action: [action_idx; 선택할 변수 인덱스 , delta; idx 변수에 대해서 변화시킬 값의 정도] \n",
    "        delta: float, 선택 변수에 더할 값\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # 변수별 min-max 범위 적용\n",
    "        var_min, var_max =col_minmax[action_idx.item()]  # 필요하면 실제 min-max 값 사용\n",
    "\n",
    "        # reward 계산: scoring model 통과\n",
    "        old_state_tensor = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            old_reward = self.score_model(old_state_tensor)  #이전 시기의 state -> reward계산\n",
    "        \n",
    "        \n",
    "        self.state[action_idx.item()]= np.clip(self.state[action_idx.item()]+ delta.item(), var_min, var_max)  # action으로 state update\n",
    "\n",
    "        new_state_tensor = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)  #new state -> reward계산\n",
    "        with torch.no_grad():\n",
    "            new_reward = self.score_model(new_state_tensor)\n",
    "\n",
    "        self.steps += 1\n",
    "        self.done = self.steps >= self.max_steps\n",
    "        return self.state.copy(), alpha*(old_reward-new_reward), self.done   #업데이트된 state 반환, dbs score의 감소량에 비례하는 보상 -> dbs score감소를 촉진, episode 끝났는지 반환\n",
    "\n",
    "    def state_dim(self):\n",
    "        return self.num_features\n",
    "\n",
    "    def action_dim(self):\n",
    "        return self.num_features  # 34개 변수 중 1개 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "c4bd6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class HybridActor(nn.Module):\n",
    "    def __init__(self, state_dim=34, discrete_dim=34):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.discrete_dim = discrete_dim\n",
    "\n",
    "        # Shared encoder\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Discrete head (variable index 선택)\n",
    "        self.discrete_head = nn.Linear(64, discrete_dim)\n",
    "        \n",
    "\n",
    "        # Delta head: state + one-hot(index) 입력\n",
    "        # input dim = state_dim + discrete_dim\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + discrete_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        h = self.shared(state)\n",
    "        logits = self.discrete_head(h)\n",
    "        dist = Categorical(logits=logits) \n",
    "        action_index = dist.sample()   #34개의 변수에 대한 logit값을 얻은 후에, 이 logit으로 dist 만들어서 index 정수값 샘플링한다. (후반에 별로 -> epsilon-greedy?)\n",
    "        return action_index\n",
    "\n",
    "    def compute_delta(self, state, action_index):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)  # [1, state_dim]\n",
    "\n",
    "        # action_index도 1차원으로 맞춤\n",
    "        if action_index.dim() == 0:   # 스칼라\n",
    "            action_index = action_index.unsqueeze(0)  # [1]\n",
    "        # One-hot 인코딩\n",
    "        onehot = F.one_hot(action_index.long(), num_classes=self.discrete_dim).float()\n",
    "\n",
    "        # concat\n",
    "        x = torch.cat([state, onehot], dim=1)\n",
    "\n",
    "        # delta 예측\n",
    "        delta = self.delta_net(x)\n",
    "        var_min, var_max =col_minmax[action_index.item()]\n",
    "        delta = torch.clamp(delta, min=var_min, max=var_max)\n",
    "        return delta\n",
    "\n",
    "\n",
    "class HybridCritic(nn.Module):\n",
    "    def __init__(self, state_dim=34, discrete_dim=34):\n",
    "        super().__init__()\n",
    "        self.discrete_dim=discrete_dim\n",
    "        \n",
    "        # Input dim = state + onehot(index) + delta\n",
    "        input_dim = state_dim + discrete_dim + 1\n",
    "        \n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action_index, delta):\n",
    "        # action_index : [B] long\n",
    "        # delta : [B, 1] continuous\n",
    "\n",
    "        onehot = F.one_hot(action_index, num_classes=self.discrete_dim).float()\n",
    "        if state.ndim == 1:\n",
    "            state = state.unsqueeze(0)  # [1, state_dim]\n",
    "        if onehot.ndim == 1:\n",
    "            onehot = onehot.unsqueeze(0)  # [1, discrete_dim]\n",
    "\n",
    "        # delta\n",
    "        if delta.ndim == 0:\n",
    "            delta = delta.unsqueeze(0).unsqueeze(1)  # [1,1]\n",
    "        elif delta.ndim == 1:\n",
    "            delta = delta.unsqueeze(1)  # [batch, 1]\n",
    "        x = torch.cat([state, onehot, delta], dim=1)\n",
    "        q = self.q_net(x)\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "9b8358de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, actor, critic, epochs=10, gamma=0.99, lr=1e-3):\n",
    "    actor_opt = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_opt = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for pid in env.patient_data.keys():\n",
    "            for ep in range(env.max_steps):\n",
    "                state = env.reset(pid)\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "                    # ============================\n",
    "                    # 1) ACTOR FORWARD (with grad)\n",
    "                    # ============================\n",
    "                    action_idx = actor(state_tensor)\n",
    "                    delta = actor.compute_delta(state_tensor, action_idx)\n",
    "                    \n",
    "                    # ENV STEP\n",
    "                    next_state, reward, done = env.step(action_idx, delta, alpha=100)\n",
    "\n",
    "                    # ============================\n",
    "                    # 2) CRITIC UPDATE (with grad)\n",
    "                    # ============================\n",
    "                    value = critic(state_tensor, action_idx.detach(), delta.detach())\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        next_value = critic(\n",
    "                            torch.tensor(next_state, dtype=torch.float32),\n",
    "                            action_idx.detach(),\n",
    "                            delta.detach()\n",
    "                        )\n",
    "                        target = reward + gamma * next_value * (1 - done)\n",
    "\n",
    "                    critic_loss = (value - target) ** 2\n",
    "\n",
    "                    critic_opt.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    critic_opt.step()\n",
    "\n",
    "                    # ============================\n",
    "                    # 3) ACTOR UPDATE (NEW FORWARD!!)\n",
    "                    # ============================\n",
    "\n",
    "                    # 새로운 forward (actor-only graph)\n",
    "                    new_action_idx = actor(state_tensor)\n",
    "                    new_delta = actor.compute_delta(state_tensor, new_action_idx)\n",
    "\n",
    "                    # Advantage는 오직 scalar 상수만 사용 (no grad)\n",
    "                    with torch.no_grad():\n",
    "                        value_now = critic(state_tensor, new_action_idx.detach(), new_delta.detach())\n",
    "                        advantage = (target - value_now)\n",
    "\n",
    "                    # deterministic policy → gradient 는 delta만 통해 진행\n",
    "                    log_prob = new_delta  \n",
    "\n",
    "                    actor_loss = -log_prob * advantage\n",
    "\n",
    "                    actor_opt.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_opt.step()\n",
    "\n",
    "                    state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\2224514807.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1610385141.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  old_state_tensor = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1610385141.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.state[action_idx.item()]= np.clip(self.state[action_idx.item()]+ delta.item(), var_min, var_max)  # action으로 state update\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1610385141.py:42: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self.state[action_idx.item()]= np.clip(self.state[action_idx.item()]+ delta.item(), var_min, var_max)  # action으로 state update\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1610385141.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_state_tensor = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)  #new state -> reward계산\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\2224514807.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  torch.tensor(next_state, dtype=torch.float32),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[548]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m actor = HybridActor()\n\u001b[32m     10\u001b[39m critic = HybridCritic()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[547]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(env, actor, critic, epochs, gamma, lr)\u001b[39m\n\u001b[32m     18\u001b[39m delta = actor.compute_delta(state_tensor, action_idx)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ENV STEP\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m next_state, reward, done = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 2) CRITIC UPDATE (with grad)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m     26\u001b[39m value = critic(state_tensor, action_idx.detach(), delta.detach())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[545]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mPatientEnv.step\u001b[39m\u001b[34m(self, action_idx, delta, alpha)\u001b[39m\n\u001b[32m     37\u001b[39m old_state_tensor = torch.tensor(\u001b[38;5;28mself\u001b[39m.state, dtype=torch.float32).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     old_reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_state_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#이전 시기의 state -> reward계산\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.state[action_idx.item()]= np.clip(\u001b[38;5;28mself\u001b[39m.state[action_idx.item()]+ delta.item(), var_min, var_max)  \u001b[38;5;66;03m# action으로 state update\u001b[39;00m\n\u001b[32m     44\u001b[39m new_state_tensor = torch.tensor(\u001b[38;5;28mself\u001b[39m.state, dtype=torch.float32).unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m#new state -> reward계산\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[544]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mscoring\u001b[39m\u001b[34m(state_input)\u001b[39m\n\u001b[32m      5\u001b[39m     X_patient_t = X_patient_t.unsqueeze(\u001b[32m0\u001b[39m) \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mload_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTotalModel\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model.eval()\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[543]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mload_best_model\u001b[39m\u001b[34m(model, device)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_best_model\u001b[39m(model, device=device):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mbest_model.pt\u001b[39m\u001b[33m\"\u001b[39m, map_location=device))\n\u001b[32m     17\u001b[39m     model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cmc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cmc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cmc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cmc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cmc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cmc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 환자 데이터 로드\n",
    "num_patients = newdf.shape[0]\n",
    "num_features = newdf.shape[1]\n",
    "patient_data = {\n",
    "    i: {\"features\": X.iloc[i], \"label\": y.iloc[i]} for i in range(newdf.shape[0])\n",
    "}\n",
    "\n",
    "env = PatientEnv(patient_data, scoring, max_steps=8)\n",
    "actor = HybridActor()\n",
    "critic = HybridCritic()\n",
    "\n",
    "train_ppo(env, actor, critic, epochs=2, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0aac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, actor, pid, max_steps=8,print_mode=True):\n",
    "    # action index → 문자열 매핑\n",
    "    feature_names = [\n",
    "        'gender','age','race','educ','marry','house','pov','wt','ht',\n",
    "        'bmi','wst','hip','dia','pulse','sys','alt','albumin','ast',\n",
    "        'crea','chol','tyg','ggt','wbc','hb','hct','ldl','hdl',\n",
    "        'acratio','glu','insulin','crp','hb1ac','mvpa','ac_week'\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    state = env.reset(pid)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_log = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # === ACTOR 선택 ===\n",
    "        action_idx = actor(state_tensor)\n",
    "        delta = actor.compute_delta(state_tensor, action_idx)\n",
    "\n",
    "        # === ENV STEP ===\n",
    "        next_state, reward, done = env.step(action_idx, delta, alpha=5)\n",
    "        total_reward += reward\n",
    "\n",
    "        # 기록 저장\n",
    "        entry = {\n",
    "            \"step\": step + 1,\n",
    "            \"action_feature\": feature_names[int(action_idx)],\n",
    "            \"action_idx\": int(action_idx),\n",
    "            \"delta\": float(delta),\n",
    "            \"reward\": float(reward)\n",
    "        }\n",
    "        episode_log.append(entry)\n",
    "        if print_mode:\n",
    "            print(f\"Step {entry['step']}:\")\n",
    "            print(f\"  Action Feature : {entry['action_feature']}\")\n",
    "            print(f\"  Delta          : {entry['delta']:.9f}\")\n",
    "            print(f\"  Reward         : {entry['reward']:.9f}\")\n",
    "            print(\"\")  # 줄바꿈\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    results[pid] = {\n",
    "        \"total_reward\": float(total_reward),\n",
    "        \"episode\": episode_log\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b18e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "  Action Feature : wt\n",
      "  Delta          : 42.799999237\n",
      "  Reward         : 0.000141488\n",
      "\n",
      "Step 2:\n",
      "  Action Feature : hip\n",
      "  Delta          : 77.800003052\n",
      "  Reward         : -0.000039927\n",
      "\n",
      "Step 3:\n",
      "  Action Feature : wt\n",
      "  Delta          : 42.799999237\n",
      "  Reward         : 0.000064697\n",
      "\n",
      "Step 4:\n",
      "  Action Feature : wt\n",
      "  Delta          : 42.799999237\n",
      "  Reward         : 0.000021472\n",
      "\n",
      "Step 5:\n",
      "  Action Feature : glu\n",
      "  Delta          : 59.000000000\n",
      "  Reward         : 0.000770478\n",
      "\n",
      "Step 6:\n",
      "  Action Feature : alt\n",
      "  Delta          : 5.000000000\n",
      "  Reward         : 0.000001501\n",
      "\n",
      "Step 7:\n",
      "  Action Feature : alt\n",
      "  Delta          : 5.000000000\n",
      "  Reward         : 0.000001450\n",
      "\n",
      "Step 8:\n",
      "  Action Feature : hb\n",
      "  Delta          : 9.800000191\n",
      "  Reward         : 0.000000254\n",
      "\n",
      "Step 9:\n",
      "  Action Feature : alt\n",
      "  Delta          : 5.000000000\n",
      "  Reward         : 0.000001367\n",
      "\n",
      "Step 10:\n",
      "  Action Feature : hb\n",
      "  Delta          : 9.800000191\n",
      "  Reward         : 0.000000000\n",
      "\n",
      "{97: {'total_reward': 0.0009627800920952723, 'episode': [{'step': 1, 'action_feature': 'wt', 'action_idx': 7, 'delta': 42.79999923706055, 'reward': 0.0001414877033312223}, {'step': 2, 'action_feature': 'hip', 'action_idx': 11, 'delta': 77.80000305175781, 'reward': -3.9927272155182436e-05}, {'step': 3, 'action_feature': 'wt', 'action_idx': 7, 'delta': 42.79999923706055, 'reward': 6.469673508036067e-05}, {'step': 4, 'action_feature': 'wt', 'action_idx': 7, 'delta': 42.79999923706055, 'reward': 2.147208988390048e-05}, {'step': 5, 'action_feature': 'glu', 'action_idx': 28, 'delta': 59.0, 'reward': 0.0007704781488371282}, {'step': 6, 'action_feature': 'alt', 'action_idx': 15, 'delta': 5.0, 'reward': 1.5011956122634729e-06}, {'step': 7, 'action_feature': 'alt', 'action_idx': 15, 'delta': 5.0, 'reward': 1.4499121903099876e-06}, {'step': 8, 'action_feature': 'hb', 'action_idx': 23, 'delta': 9.800000190734863, 'reward': 2.5428903427382465e-07}, {'step': 9, 'action_feature': 'alt', 'action_idx': 15, 'delta': 5.0, 'reward': 1.3672902809958032e-06}, {'step': 10, 'action_feature': 'hb', 'action_idx': 23, 'delta': 9.800000190734863, 'reward': 0.0}]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\3193760226.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1895905361.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  old_state_tensor = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1895905361.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.state[action_idx.item()]= np.clip(self.state[action_idx.item()]+ delta.item(), var_min, var_max)  # action으로 state update\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1895905361.py:42: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self.state[action_idx.item()]= np.clip(self.state[action_idx.item()]+ delta.item(), var_min, var_max)  # action으로 state update\n",
      "C:\\Users\\cmc\\AppData\\Local\\Temp\\ipykernel_38520\\1895905361.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_state_tensor = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)  #new state -> reward계산\n"
     ]
    }
   ],
   "source": [
    "test_results = test_agent(env,actor, max_steps=8,pid=97)\n",
    "print(test_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
